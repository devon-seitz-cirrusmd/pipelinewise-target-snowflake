import io
import json
import unittest
import os
import itertools
from types import SimpleNamespace

from contextlib import redirect_stdout
from datetime import datetime, timedelta
from unittest.mock import patch, mock_open

import target_snowflake


def _mock_record_to_csv_line(record):
    return record


class TestTargetSnowflake(unittest.TestCase):
    def setUp(self):
        self.config = {}
        self.maxDiff = None

    @patch("target_snowflake.flush_streams")
    @patch("target_snowflake.DbSync")
    def test_persist_lines_with_40_records_and_batch_size_of_20_expect_flushing_once(
        self, dbSync_mock, flush_streams_mock
    ):
        self.config["batch_size_rows"] = 20
        self.config["flush_all_streams"] = True

        with open(
            f"{os.path.dirname(__file__)}/resources/logical-streams.json", "r"
        ) as f:
            lines = f.readlines()

        instance = dbSync_mock.return_value
        instance.create_schema_if_not_exists.return_value = None
        instance.sync_table.return_value = None

        flush_streams_mock.return_value = '{"currently_syncing": null}'

        target_snowflake.persist_lines(self.config, lines)

        self.assertEqual(1, flush_streams_mock.call_count)

    @patch("target_snowflake.current_memory_consumption_percentage")
    @patch("target_snowflake.flush_streams")
    @patch("target_snowflake.DbSync")
    @patch("target_snowflake.MAX_MEMORY_THRESHOLD_CHECK_EVERY_N_ROWS", 1)
    def test_persist_lines_with_memory_threshold_reached_expect_multiple_flushings(
        self,
        dbSync_mock,
        flush_streams_mock,
        current_memory_consumption_percentage_mock,
    ):
        self.config["batch_size_rows"] = 20
        self.config["max_memory_threshold"] = 0.1

        with open(
            f"{os.path.dirname(__file__)}/resources/same-schemas-multiple-times.json",
            "r",
        ) as f:
            lines = f.readlines()

        current_memory_consumption_percentage_mock.return_value = 0.2
        instance = dbSync_mock.return_value
        instance.create_schema_if_not_exists.return_value = None
        instance.sync_table.return_value = None

        flush_streams_mock.return_value = '{"currently_syncing": null}'

        target_snowflake.persist_lines(self.config, lines)

        self.assertEqual(5, flush_streams_mock.call_count)

    @patch("target_snowflake.flush_streams")
    @patch("target_snowflake.DbSync")
    def test_persist_lines_with_same_schema_expect_flushing_once(
        self, dbSync_mock, flush_streams_mock
    ):
        self.config["batch_size_rows"] = 20

        with open(
            f"{os.path.dirname(__file__)}/resources/same-schemas-multiple-times.json",
            "r",
        ) as f:
            lines = f.readlines()

        instance = dbSync_mock.return_value
        instance.create_schema_if_not_exists.return_value = None
        instance.sync_table.return_value = None

        flush_streams_mock.return_value = '{"currently_syncing": null}'

        target_snowflake.persist_lines(self.config, lines)

        self.assertEqual(1, flush_streams_mock.call_count)

    @patch("target_snowflake.datetime")
    @patch("target_snowflake.flush_streams")
    @patch("target_snowflake.DbSync")
    def test_persist_40_records_with_batch_wait_limit(
        self, dbSync_mock, flush_streams_mock, dateTime_mock
    ):

        start_time = datetime(2021, 4, 6, 0, 0, 0)
        increment = 11
        counter = itertools.count()

        # Move time forward by {{increment}} seconds every time utcnow() is called
        dateTime_mock.utcnow.side_effect = lambda: start_time + timedelta(
            seconds=increment * next(counter)
        )

        self.config["batch_size_rows"] = 100
        self.config["batch_wait_limit_seconds"] = 10
        self.config["flush_all_streams"] = True

        # Expecting 40 records
        with open(
            f"{os.path.dirname(__file__)}/resources/logical-streams.json", "r"
        ) as f:
            lines = f.readlines()

        instance = dbSync_mock.return_value
        instance.create_schema_if_not_exists.return_value = None
        instance.sync_table.return_value = None

        flush_streams_mock.return_value = '{"currently_syncing": null}'

        target_snowflake.persist_lines(self.config, lines)

        # Expecting flush after every records + 1 at the end
        self.assertEqual(flush_streams_mock.call_count, 41)

    @patch("target_snowflake.DbSync")
    @patch("target_snowflake.os.remove")
    def test_archive_load_files_incremental_replication(
        self, os_remove_mock, dbSync_mock
    ):
        self.config["tap_id"] = "test_tap_id"
        self.config["archive_load_files"] = True
        self.config["s3_bucket"] = "dummy_bucket"

        with open(
            f"{os.path.dirname(__file__)}/resources/messages-simple-table.json", "r"
        ) as f:
            lines = f.readlines()

        instance = dbSync_mock.return_value
        instance.create_schema_if_not_exists.return_value = None
        instance.sync_table.return_value = None
        instance.put_to_stage.return_value = (
            "some-s3-folder/some-name_date_batch_hash.csg.gz"
        )

        target_snowflake.persist_lines(self.config, lines)

        copy_to_archive_args = instance.copy_to_archive.call_args[0]
        self.assertEqual(
            copy_to_archive_args[0], "some-s3-folder/some-name_date_batch_hash.csg.gz"
        )
        self.assertEqual(
            copy_to_archive_args[1],
            "test_tap_id/test_simple_table/some-name_date_batch_hash.csg.gz",
        )
        self.assertDictEqual(
            copy_to_archive_args[2],
            {
                "tap": "test_tap_id",
                "schema": "tap_mysql_test",
                "table": "test_simple_table",
                "archived-by": "pipelinewise_target_snowflake",
                "incremental-key": "id",
                "incremental-key-min": "1",
                "incremental-key-max": "5",
            },
        )

    @patch("target_snowflake.DbSync")
    @patch("target_snowflake.os.remove")
    def test_archive_load_files_log_based_replication(
        self, os_remove_mock, dbSync_mock
    ):
        self.config["tap_id"] = "test_tap_id"
        self.config["archive_load_files"] = True

        with open(
            f"{os.path.dirname(__file__)}/resources/logical-streams.json", "r"
        ) as f:
            lines = f.readlines()

        instance = dbSync_mock.return_value
        instance.create_schema_if_not_exists.return_value = None
        instance.sync_table.return_value = None
        instance.put_to_stage.return_value = (
            "some-s3-folder/some-name_date_batch_hash.csg.gz"
        )

        target_snowflake.persist_lines(self.config, lines)

        copy_to_archive_args = instance.copy_to_archive.call_args[0]
        self.assertEqual(
            copy_to_archive_args[0], "some-s3-folder/some-name_date_batch_hash.csg.gz"
        )
        self.assertEqual(
            copy_to_archive_args[1],
            "test_tap_id/logical1_table2/some-name_date_batch_hash.csg.gz",
        )
        self.assertDictEqual(
            copy_to_archive_args[2],
            {
                "tap": "test_tap_id",
                "schema": "logical1",
                "table": "logical1_table2",
                "archived-by": "pipelinewise_target_snowflake",
            },
        )

    @patch("target_snowflake.flush_streams")
    @patch("target_snowflake.DbSync")
    def test_persist_lines_with_only_state_messages(
        self, dbSync_mock, flush_streams_mock
    ):
        """
        Given only state messages, target should emit the last one
        """

        self.config["batch_size_rows"] = 5

        with open(
            f"{os.path.dirname(__file__)}/resources/streams_only_state.json", "r"
        ) as f:
            lines = f.readlines()

        instance = dbSync_mock.return_value
        instance.create_schema_if_not_exists.return_value = None
        instance.sync_table.return_value = None

        # catch stdout
        buf = io.StringIO()
        with redirect_stdout(buf):
            target_snowflake.persist_lines(self.config, lines)

        flush_streams_mock.assert_not_called()

        self.assertEqual(
            buf.getvalue().strip(),
            '{"bookmarks": {"tap_mysql_test-test_simple_table": {"replication_key": "id", '
            '"replication_key_value": 100, "version": 1}}}',
        )

    def test_current_memory_consumption_percentage_cgroup(self):
        with patch(
            "builtins.open", mock_open(read_data="20"), create=True
        ) as mock_builtin_open:
            assert target_snowflake.current_memory_consumption_percentage() == 1.0

    @patch("psutil.virtual_memory")
    def test_current_memory_consumption_percentage_psutil(
        self, psutil_virtual_memory_mock
    ):
        vmem = SimpleNamespace()
        vmem.available = 20
        vmem.total = 100
        psutil_virtual_memory_mock.return_value = vmem
        assert target_snowflake.current_memory_consumption_percentage() == 0.8
